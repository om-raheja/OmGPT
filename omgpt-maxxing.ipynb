{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d6386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:21.905026Z",
     "iopub.status.busy": "2024-10-15T16:26:21.904686Z",
     "iopub.status.idle": "2024-10-15T16:26:21.909118Z",
     "shell.execute_reply": "2024-10-15T16:26:21.908284Z"
    },
    "papermill": {
     "duration": 0.011646,
     "end_time": "2024-10-15T16:26:21.911114",
     "exception": false,
     "start_time": "2024-10-15T16:26:21.899468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# We always start with a dataset to train on. Let's download the tiny shakespe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb152786",
   "metadata": {
    "papermill": {
     "duration": 0.003277,
     "end_time": "2024-10-15T16:26:21.918087",
     "exception": false,
     "start_time": "2024-10-15T16:26:21.914810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I use a smaller dataset here so that we can prevent overfitting. We can add a check that if train loss exceeds validation loss by >1, we need to increase dropout rate or other checks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddaba3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:21.926588Z",
     "iopub.status.busy": "2024-10-15T16:26:21.926016Z",
     "iopub.status.idle": "2024-10-15T16:26:21.934577Z",
     "shell.execute_reply": "2024-10-15T16:26:21.933740Z"
    },
    "papermill": {
     "duration": 0.014752,
     "end_time": "2024-10-15T16:26:21.936559",
     "exception": false,
     "start_time": "2024-10-15T16:26:21.921807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datafile = \"/kaggle/input/dickens-yap/novels.txt\"\n",
    "# \"/kaggle/input/thisrandombook/data.txt\"\n",
    "# we using smaller data and building up to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343f564",
   "metadata": {
    "papermill": {
     "duration": 0.003299,
     "end_time": "2024-10-15T16:26:21.943209",
     "exception": false,
     "start_time": "2024-10-15T16:26:21.939910",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5eb82ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:21.951171Z",
     "iopub.status.busy": "2024-10-15T16:26:21.950910Z",
     "iopub.status.idle": "2024-10-15T16:26:25.149088Z",
     "shell.execute_reply": "2024-10-15T16:26:25.148315Z"
    },
    "papermill": {
     "duration": 3.204686,
     "end_time": "2024-10-15T16:26:25.151412",
     "exception": false,
     "start_time": "2024-10-15T16:26:21.946726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb7df97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:25.159807Z",
     "iopub.status.busy": "2024-10-15T16:26:25.159409Z",
     "iopub.status.idle": "2024-10-15T16:26:25.164441Z",
     "shell.execute_reply": "2024-10-15T16:26:25.163665Z"
    },
    "papermill": {
     "duration": 0.01126,
     "end_time": "2024-10-15T16:26:25.166338",
     "exception": false,
     "start_time": "2024-10-15T16:26:25.155078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----\n",
    "# hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "n_embed = 512 # dimensionality of the character embedding vectors\n",
    "n_head = 6 # number of heads in the multi-head attention\n",
    "n_layer = 6\n",
    "dropout = 0.4\n",
    "\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "# ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69f016",
   "metadata": {
    "papermill": {
     "duration": 0.003187,
     "end_time": "2024-10-15T16:26:25.173029",
     "exception": false,
     "start_time": "2024-10-15T16:26:25.169842",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7510ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:25.182431Z",
     "iopub.status.busy": "2024-10-15T16:26:25.181684Z",
     "iopub.status.idle": "2024-10-15T16:26:26.133322Z",
     "shell.execute_reply": "2024-10-15T16:26:26.132540Z"
    },
    "papermill": {
     "duration": 0.958184,
     "end_time": "2024-10-15T16:26:26.135693",
     "exception": false,
     "start_time": "2024-10-15T16:26:25.177509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():                          \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    free_gpu_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7985bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:26.145423Z",
     "iopub.status.busy": "2024-10-15T16:26:26.145122Z",
     "iopub.status.idle": "2024-10-15T16:26:27.147063Z",
     "shell.execute_reply": "2024-10-15T16:26:27.145583Z"
    },
    "papermill": {
     "duration": 1.010803,
     "end_time": "2024-10-15T16:26:27.150301",
     "exception": false,
     "start_time": "2024-10-15T16:26:26.139498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74220643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:27.162554Z",
     "iopub.status.busy": "2024-10-15T16:26:27.162161Z",
     "iopub.status.idle": "2024-10-15T16:26:27.313794Z",
     "shell.execute_reply": "2024-10-15T16:26:27.312844Z"
    },
    "papermill": {
     "duration": 0.160538,
     "end_time": "2024-10-15T16:26:27.315808",
     "exception": false,
     "start_time": "2024-10-15T16:26:27.155270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# ----\n",
    "\n",
    "# create a model\n",
    "class BiGramDataModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        # initialize with random weights\n",
    "        super().__init__()\n",
    "        # warning: generally set to a small value but we have characters for tokens\n",
    "        # this basically is used to tell you what the probability is of one token\n",
    "        # coming after another, hence \"bigram\"\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "                *[Block(n_embed, n_head) for _ in range(n_layer)],\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T= idx.shape\n",
    "        idx = idx\n",
    "        # plugs the tokens into the table \n",
    "        # semantic lationship, this is basically the \"self-attention\" part \n",
    "        # of the paper, and these complex weights are the only thing that's\n",
    "        # really trained here :)\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        # add them\n",
    "        x = token_emb + pos_emb\n",
    "        # apply the head\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # apply lm head (linear transformation to return back to life)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if there's nothing we compare to\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # batch size, sequence length, classes/dimensions\n",
    "            # number of sequences being processed simultaneously WHY DO WE CARE\n",
    "            # number of time steps/token per sequence (block_size)\n",
    "            # contains informatation about the tokens before it, the \\\n",
    "            # \"density\" of each token\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # view it as a 2D tensor of what all has been processed \n",
    "            # that way we can have entropy\n",
    "            logits = logits.view(B * T, C)\n",
    "\n",
    "            # targets is the same thing except there is no output size\n",
    "            # they don't care about storing context of each token in \n",
    "            # the output\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            # quantifying the information encoded between what it is\n",
    "            # and what the semantic relationship should identify\n",
    "            # warning: I don't really understand what this does\n",
    "            # also can the same text have different semantic relationship?\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            # this calls forward for the tokens \n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            # remove T because this is a BiGram model\n",
    "            # this might be wrong\n",
    "            logits = logits[:, -1, :]  # get the last time step \n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)  # probabilities\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1).\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# create a Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        # initialize with random weights\n",
    "        super().__init__()\n",
    "        # add a key, value, and query\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute attention scores\n",
    "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 #normalization\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, size):\n",
    "        super().__init__()\n",
    "        self.head_list = nn.ModuleList([Head(size) for _ in range(heads)])\n",
    "        self.proj = nn.Linear(heads * size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.head_list], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embed * 4, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed)\n",
    "        \n",
    "        # batch normalization\n",
    "        # https://arxiv.org/abs/1607.06450\n",
    "        self.la1 = nn.LayerNorm(n_embed)\n",
    "        self.la2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        # some optimization by adding it\n",
    "        x = x + self.sa_heads(self.la1(x)) # this is the same as x = x + self.sa_heads(x)\n",
    "        x = x + self.ff(self.la2(x)) # this is the same as x = x + self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9640784c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-15T16:26:27.324826Z",
     "iopub.status.busy": "2024-10-15T16:26:27.324476Z",
     "iopub.status.idle": "2024-10-15T17:55:03.085879Z",
     "shell.execute_reply": "2024-10-15T17:55:03.084803Z"
    },
    "papermill": {
     "duration": 5315.768223,
     "end_time": "2024-10-15T17:55:03.088044",
     "exception": false,
     "start_time": "2024-10-15T16:26:27.319821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2725304 characters, 86 unique\n",
      "inputs:\n",
      "torch.Size([64, 256])\n",
      "tensor([[44, 83, 83,  ..., 78, 50, 14],\n",
      "        [83, 69, 36,  ..., 17, 24, 17],\n",
      "        [50, 24, 25,  ..., 69, 26, 62],\n",
      "        ...,\n",
      "        [50, 14, 62,  ..., 36, 50, 44],\n",
      "        [24, 17, 62,  ..., 78, 69, 36],\n",
      "        [50, 41, 19,  ...,  9, 36, 29]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 256])\n",
      "tensor([[83, 83, 50,  ..., 50, 14, 44],\n",
      "        [69, 36, 50,  ..., 24, 17,  5],\n",
      "        [24, 25, 53,  ..., 26, 62, 50],\n",
      "        ...,\n",
      "        [14, 62, 50,  ..., 50, 44, 19],\n",
      "        [17, 62, 50,  ..., 69, 36, 60],\n",
      "        [41, 19,  5,  ..., 36, 29, 50]], device='cuda:0')\n",
      "torch.Size([16384, 86]) tensor(4.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "qpêX]t)7ugs“zyQ0sgzi\n",
      "EJY“fekpQe‘7RE!o\n",
      "r2q;WeqGefjOh8NldG2TvGnq!Y6tUOAGxoh‘Q,Qx]()NYr(-gL8H\n",
      "kg\n",
      "g12’8X\"\n",
      "step 0: train loss 4.6199, val loss 4.6154\n",
      "step 500: train loss 2.3242, val loss 2.3097\n",
      "step 1000: train loss 1.9196, val loss 1.9247\n",
      "step 1500: train loss 1.7320, val loss 1.7372\n",
      "step 2000: train loss 1.6087, val loss 1.6227\n",
      "step 2500: train loss 1.5228, val loss 1.5435\n",
      "step 3000: train loss 1.4564, val loss 1.4835\n",
      "step 3500: train loss 1.3996, val loss 1.4338\n",
      "step 4000: train loss 1.3606, val loss 1.3996\n",
      "step 4500: train loss 1.3264, val loss 1.3725\n",
      "step 5000: train loss 1.2958, val loss 1.3443\n",
      "step 5500: train loss 1.2715, val loss 1.3287\n",
      "step 6000: train loss 1.2513, val loss 1.3117\n",
      "step 6500: train loss 1.2319, val loss 1.2965\n",
      "step 7000: train loss 1.2150, val loss 1.2855\n",
      "step 7500: train loss 1.1979, val loss 1.2702\n",
      "step 8000: train loss 1.1868, val loss 1.2639\n",
      "step 8500: train loss 1.1740, val loss 1.2551\n",
      "step 9000: train loss 1.1587, val loss 1.2452\n",
      "step 9500: train loss 1.1475, val loss 1.2372\n",
      "step 9999: train loss 1.1415, val loss 1.2318\n",
      "quiry, to gow any other, than the knip was not longs at the bewifi. Husher, How many of mence of Sevices were being existened out. Mr. Weller of the chaise of Mr. Bob Sawyer was need, at the fagust beath the maid. He has dried, and red the landing with determined to the Bob Sawyer.\n",
      "\n",
      "‘Yes, ye,’ said Samivel, glaned at the earline the end, in him. ‘Will generally; or with they’re so rady to the subberve benth a special up, ma’am, sir,’ intervate the gentleman about.\n",
      "\n",
      "‘Your know—I am,’ said Mr. Well\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# warning: susceptible to overlapping data\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# tokenization\n",
    "data = \"\"\n",
    "with open(datafile, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# tokens \n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, vocab_size))\n",
    "\n",
    "m = BiGramDataModel(vocab_size)\n",
    "m = m.to(device)\n",
    "\n",
    "# mapping\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "## encode and decode\n",
    "encode = lambda x: [char_to_ix[c] for c in x]\n",
    "decode = lambda x: \"\".join([ix_to_char[i] for i in x])\n",
    "\n",
    "# convert the data to numbers\n",
    "data_num = torch.tensor(encode(data), dtype=torch.long).to(device)\n",
    "\n",
    "## decide training and validation data\n",
    "n = int(0.9 * len(data_num))\n",
    "train_data = data_num[:n]\n",
    "val_data = data_num[n:]\n",
    "\n",
    "x = train_data[:block_size]  # input\n",
    "y = train_data[1:block_size + 1]  # labels\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "# print untrained output for testing\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "# optimize \n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    # hangs over here somewhere no idea why\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if steps % eval_interval == 0 or steps == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # approach the optimized gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "## generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist())) # print somewhat trained data\n",
    "\n",
    "# pickle the results\n",
    "torch.save((vocab_size, char_to_ix, ix_to_char, m.state_dict()), \"model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5842937,
     "sourceId": 9582194,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5847524,
     "sourceId": 9588238,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5881465,
     "sourceId": 9633445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5325.068163,
   "end_time": "2024-10-15T17:55:04.316388",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-15T16:26:19.248225",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
